{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da5bc7f6-fe32-4f13-949e-3f955e4b4a4e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07766749-abfd-4afc-b2ee-376c5ee76715",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wmfdata as wmf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc2e90ce-2ccc-44e5-82ee-cdd94e01c595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6df215a-7385-45f7-9a76-1cc7de4206a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = None\n",
    "bold = '\\033[1m'\n",
    "end = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb2de01-e418-4d4a-aee6-52251e0a0289",
   "metadata": {
    "tags": []
   },
   "source": [
    "## spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70f332f-ce3c-47ee-9b07-a5028a511611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no active session\n"
     ]
    }
   ],
   "source": [
    "spark_session = wmf.spark.get_active_session()\n",
    "\n",
    "if type(spark_session) != type(None):\n",
    "    spark_session.stop()\n",
    "else:\n",
    "    print('no active session')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0283af2a-ee0a-43e7-be6f-96644e5b67ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /usr/lib/spark3\n",
      "Using Hadoop client lib jars at 3.2.0, provided by Spark.\n",
      "PYSPARK_PYTHON=/opt/conda-analytics/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/21 08:22:38 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12000. Attempting port 12001.\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12001. Attempting port 12002.\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12002. Attempting port 12003.\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12003. Attempting port 12004.\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12004. Attempting port 12005.\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12005. Attempting port 12006.\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12006. Attempting port 12007.\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12007. Attempting port 12008.\n",
      "23/07/21 08:22:38 WARN Utils: Service 'sparkDriver' could not bind on port 12008. Attempting port 12009.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4043. Attempting port 4044.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4044. Attempting port 4045.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4045. Attempting port 4046.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4046. Attempting port 4047.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4047. Attempting port 4048.\n",
      "23/07/21 08:22:39 WARN Utils: Service 'SparkUI' could not bind on port 4048. Attempting port 4049.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13000. Attempting port 13001.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13001. Attempting port 13002.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13002. Attempting port 13003.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13003. Attempting port 13004.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13004. Attempting port 13005.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13005. Attempting port 13006.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13006. Attempting port 13007.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13007. Attempting port 13008.\n",
      "23/07/21 08:22:49 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13008. Attempting port 13009.\n",
      "23/07/21 08:22:49 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n"
     ]
    }
   ],
   "source": [
    "spark_session = wmf.spark.create_custom_session(\n",
    "    master=\"yarn\",\n",
    "    app_name='bot-vandal-reverts',\n",
    "    spark_config={\n",
    "        \"spark.driver.memory\": \"4g\",\n",
    "        \"spark.dynamicAllocation.maxExecutors\": 64,\n",
    "        \"spark.executor.memory\": \"20g\",\n",
    "        \"spark.executor.cores\": 4,\n",
    "        \"spark.sql.shuffle.partitions\": 256,\n",
    "        \"spark.driver.maxResultSize\": \"2g\"\n",
    "        \n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ded53def-e890-490b-a012-d73d9c9e4fb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://stat1005.eqiad.wmnet:4049\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>bot-vandal-reverts</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8cc58d4340>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ig_warn()\n",
    "# spark_session = wmf.spark.create_session(type='yarn-large')\n",
    "spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fcec281-6860-4655-893e-a1016b185bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7c3d6e-7bec-45d9-89da-f680b7968fd5",
   "metadata": {},
   "source": [
    "## data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06d75af3-30bf-401d-9568-b0ca0a937b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bots = {\n",
    "    'enwiki': 'ClueBot NG',\n",
    "    'eswiki': 'SeroBOT',\n",
    "    'frwiki': 'Salebot',\n",
    "    'ptwiki': 'Salebot',\n",
    "    'fawiki': 'Dexbot',\n",
    "    'bgwiki': 'PSS 9',\n",
    "    'simplewiki': 'ChenzwBot',\n",
    "    'ruwiki': 'Рейму Хакурей',\n",
    "    'rowiki': 'PatrocleBot'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0121d3e6-5d24-449f-910d-b8e0a32c9aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/21 10:23:53 ERROR YarnScheduler: Lost executor 200 on an-worker1100.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:24:26 ERROR YarnScheduler: Lost executor 168 on an-worker1109.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.0 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:24:51 ERROR YarnScheduler: Lost executor 180 on an-worker1109.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:24:53 ERROR YarnScheduler: Lost executor 216 on an-worker1097.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.2 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:24:59 ERROR YarnScheduler: Lost executor 218 on an-worker1097.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.8 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:04 ERROR YarnScheduler: Lost executor 172 on an-worker1109.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.8 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:05 ERROR YarnScheduler: Lost executor 183 on an-worker1123.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:14 ERROR YarnScheduler: Lost executor 195 on an-worker1098.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.6 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:17 ERROR YarnScheduler: Lost executor 169 on an-worker1118.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.2 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:19 ERROR YarnScheduler: Lost executor 176 on an-worker1123.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.2 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:23 ERROR YarnScheduler: Lost executor 196 on an-worker1098.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.2 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:33 ERROR YarnScheduler: Lost executor 179 on an-worker1123.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 19.0 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:40 ERROR YarnScheduler: Lost executor 212 on an-worker1099.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.1 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:40 ERROR YarnScheduler: Lost executor 174 on an-worker1118.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.5 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:50 ERROR YarnScheduler: Lost executor 177 on an-worker1123.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.0 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:25:54 ERROR YarnScheduler: Lost executor 185 on an-worker1125.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:26:23 ERROR YarnScheduler: Lost executor 173 on an-worker1118.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.6 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:26:23 ERROR YarnScheduler: Lost executor 202 on an-worker1096.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:26:34 ERROR YarnScheduler: Lost executor 201 on an-worker1096.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:26:38 ERROR YarnScheduler: Lost executor 210 on an-worker1099.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.9 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:01 ERROR YarnScheduler: Lost executor 189 on an-worker1101.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:09 ERROR YarnScheduler: Lost executor 198 on an-worker1098.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:12 ERROR YarnScheduler: Lost executor 194 on an-worker1098.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.5 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:12 ERROR YarnScheduler: Lost executor 193 on an-worker1098.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:30 ERROR YarnScheduler: Lost executor 204 on an-worker1100.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.9 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:38 ERROR YarnScheduler: Lost executor 197 on an-worker1098.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.3 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:42 ERROR YarnScheduler: Lost executor 186 on an-worker1125.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.0 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:45 ERROR YarnScheduler: Lost executor 181 on an-worker1123.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.0 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:27:48 ERROR YarnScheduler: Lost executor 214 on an-worker1096.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.1 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:28:09 ERROR YarnScheduler: Lost executor 208 on an-worker1099.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:28:19 ERROR YarnScheduler: Lost executor 190 on an-worker1101.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.1 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:28:35 ERROR YarnScheduler: Lost executor 188 on an-worker1101.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.1 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:28:42 ERROR YarnScheduler: Lost executor 234 on an-worker1123.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:28:47 ERROR YarnScheduler: Lost executor 232 on an-worker1097.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.0 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:28:48 ERROR YarnScheduler: Lost executor 205 on an-worker1100.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:28:58 ERROR YarnScheduler: Lost executor 184 on an-worker1125.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.0 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:29:05 ERROR YarnScheduler: Lost executor 236 on an-worker1123.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.0 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:29:06 ERROR YarnScheduler: Lost executor 229 on an-worker1123.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.9 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:29:10 ERROR YarnScheduler: Lost executor 223 on an-worker1100.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.5 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:29:11 ERROR YarnScheduler: Lost executor 211 on an-worker1099.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.3 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:29:12 ERROR YarnScheduler: Lost executor 222 on an-worker1096.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 18.2 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "23/07/21 10:29:13 ERROR YarnScheduler: Lost executor 239 on an-worker1118.eqiad.wmnet: Container killed by YARN for exceeding physical memory limits. 17.7 GB of 17.6 GB physical memory used. Consider boosting spark.executor.memoryOverhead.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = \"\"\"\n",
    "WITH\n",
    "    base AS (\n",
    "        SELECT\n",
    "            wiki_db,\n",
    "            revision_id,\n",
    "            event_timestamp,\n",
    "            page_namespace,\n",
    "            revision_first_identity_reverting_revision_id,\n",
    "            revision_seconds_to_identity_revert\n",
    "        FROM\n",
    "            wmf.mediawiki_history\n",
    "        WHERE\n",
    "            snapshot = '2023-06' AND\n",
    "            wiki_db IN {DBS} AND\n",
    "            event_entity = 'revision' AND\n",
    "            event_type = 'create' AND\n",
    "            revision_is_identity_reverted),\n",
    "            \n",
    "    bot_reverts AS (\n",
    "        SELECT\n",
    "            *\n",
    "            \n",
    "        FROM\n",
    "            wmf.mediawiki_history mwh\n",
    "            JOIN base ON mwh.revision_id = base.revision_first_identity_reverting_revision_id\n",
    "        WHERE\n",
    "            snapshot = '2023-06' AND\n",
    "            event_user_text IN {BOTS} AND\n",
    "            ((YEAR(base.event_timestamp) = 2022 AND MONTH(base.event_timestamp) > 6) OR\n",
    "            (YEAR(base.event_timestamp) = 2023 AND MONTH(base.event_timestamp) <= 6))\n",
    "        )\n",
    "        \n",
    "SELECT *\n",
    "FROM bot_reverts\n",
    "LIMIT 20\n",
    "\"\"\"\n",
    "\n",
    "result = wmf.spark.run(query.format(DBS=wmf.utils.sql_tuple(bots.keys()),\n",
    "                                   BOTS=wmf.utils.sql_tuple(set(bots.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e6e805-c56d-45d2-8f5b-8bcabd6006ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c0d36d-7289-44e6-b792-730292aa61e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9561fabd-5fc6-4e3e-9536-864b81042859",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
